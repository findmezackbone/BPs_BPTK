import torch
import numpy as np
import random
import itertools
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import numpy as np
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler
import sys
sys.path.append("Python") 
from BPS_init_function import BPS_BPTK 
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif']=['SimHei'] # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus']=False # 用来正常显示负号
import keyboard
from sklearn.metrics import r2_score
torch.backends.cudnn.enabled = True



# 准备数据
y1 = np.load("Python\ForwardFitNN\WithTime\Database\ForwardLabel_zzc.npy")  #输入标签数据
y2 = np.load("Python\ForwardFitNN\WithTime\Database\ForwardLabel_SG.npy")  #输入标签数据
y = np.vstack((y1,y2))

X1 = np.load("Python\ForwardFitNN\WithTime\Database\ForwardInput_zzc.npy")
X2 = np.load("Python\ForwardFitNN\WithTime\Database\ForwardInput_SG.npy")
X =  np.vstack((X1,X2))

# 数据预处理 标准化数据


def standard_transform(x):
    # 计算每个特征的均值
    mean = x.mean(dim=0)
    # 计算每个特征的标准差
    std = x.std(dim=0, unbiased=False)  # unbiased=False 相当于 numpy 的 ddof=0
    # 避免使用0的标准差
    std[std == 0] = 1
    # 进行标准化转换
    x = (x - mean) / std
    return x,mean,std

def inverse_transform(x, mean, std):
    # 将标准化后的张量乘以标准差，然后加上均值
    x = x * std + mean
    return x

scaler = StandardScaler()


X = torch.tensor(X).float()
y = torch.tensor(y).float()

X,X_mean,X_std = standard_transform(X)
y,y_mean,y_std = standard_transform(y)

# 划分训练集、验证集和测试集
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.16, random_state=20)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=20)

# 将数据转移到 GPU（如果可用



class ResNetBlock(nn.Module):
    def __init__(self, hyperparas):
        super(ResNetBlock, self).__init__()
        
        self.hidden_dim = hyperparas['hidden_dim']
        self.block_layer_nums =hyperparas['block_layer_nums']
            
        # Define layers for the function f (MLP)
        self.layers = nn.ModuleList()
        
        for _ in range(self.block_layer_nums - 1):  # -2 because we already added one layer and last layer is already defined
            self.layers.append(nn.Linear(self.hidden_dim,self.hidden_dim ))
        
        # Layer normalization
        self.layernorms = nn.ModuleList()
        for _ in range(self.block_layer_nums - 1):  # -1 because layer normalization is not applied to the last layer
            self.layernorms.append(nn.LayerNorm(self.hidden_dim))
        
    def forward(self, x):
        # Forward pass through the function f (MLP)
        out = x
        for i in range(self.block_layer_nums - 1):  # -1 because last layer is already applied outside the loop
            out = self.layers[i](out)
            out = self.layernorms[i](out)
            out = torch.relu(out)
        
        # Element-wise addition of input x and output of function f(x)
        out = x + out
        
        return out
    

class CustomResNN(nn.Module):
    def __init__(self,hyperparas):
        super().__init__()
        self.input_dim = hyperparas['input_dim'] #4
        self.hidden_dim = hyperparas['hidden_dim'] #30
        self.hidden_nums = hyperparas['hidden_nums'] #3
        self.output_dim = hyperparas['output_dim'] #2
        self.block_layer_nums = hyperparas['block_layer_nums'] #3

        self.layer_list = []
        self.layer_list.append(nn.Sequential(nn.Linear(self.input_dim,self.hidden_dim),nn.ReLU() ) )

        for _ in range(self.hidden_nums-1):
            self.layer_list.append(ResNetBlock(hyperparas))

        self.layer_list.append(nn.Linear(self.hidden_dim,self.output_dim))

        self.linear_Res_final = nn.Sequential(*self.layer_list)

    def forward(self,inputs):
        
        return self.linear_Res_final(inputs)

#超参数合集
hyperparas = {'input_dim':4,'hidden_dim':50,'hidden_nums':5,'output_dim':2,'block_layer_nums':3}
learning_rate = 0.001
num_epochs = 300


# 初始化模型、损失函数和优化器
model = CustomResNN(hyperparas)

loss_function=nn.MSELoss()

def criterion(output,label,mode = 1):
    if mode == 1:
        return loss_function(output,label)
    if mode == 2:
        output_reverse = output
        label_reverse = label
        return torch.mean(torch.abs(output_reverse - label_reverse)/torch.max(label_reverse, torch.full_like(label_reverse, 1E-9)))
    if mode == 3:
        output_reverse = output
        label_reverse = label
        loss2 = torch.mean(torch.abs(output_reverse - label_reverse)/torch.max(label_reverse, torch.full_like(label_reverse, 1E-9)))
        loss1 = loss_function(output,label)
        loss = loss1 + 0.001*loss2
        return loss
    if mode == 4:
       
        loss2 = torch.mean(torch.abs(output - label)/torch.max(label, torch.full_like(label, 1E-9)))
        loss1 = loss_function(output,label)
        
        loss1 = 7*1E7*loss1
        loss2 = loss2
       
        loss = loss1 + loss2
        return loss, loss1, loss2

# 加载效果最好的模型
best_model = CustomResNN(hyperparas)
best_model.load_state_dict(torch.load('Python\\ForwardFitNN\\Settled_Model\\TimeFourTo2\\1\\model1.pth'))

# 在测试集上评估模型
outputs = best_model(X_test)
y_test = y_test.detach().numpy()
outputs = outputs.detach().numpy()

mse = np.mean((y_test - outputs ) ** 2)
mre = np.mean(np.abs(y_test - outputs )/np.maximum(y_test, 1E-6))
R2 =  r2_score( y_test,outputs) #决定系数

print(f'整个测试集的原始标签与输出的真实变换的MSE为  {mse}')
print(f'整个测试集的原始标签与输出的真实变换的MRE为  {mre}')
print(f'整个测试集的原始标签与输出的真实变换的决定系数为  {R2}')
